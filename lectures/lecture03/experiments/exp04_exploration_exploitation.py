#!/usr/bin/env python3
"""
RL2025 - Lecture 3: Experiment 04 - Exploration vs Exploitation

This experiment demonstrates the exploration-exploitation tradeoff using different
action selection strategies in CartPole-v1. We compare purely random, purely greedy,
and epsilon-greedy policies.

Learning objectives:
- Understand the exploration vs exploitation dilemma
- Implement epsilon-greedy action selection
- Compare performance of different exploration strategies
- Analyze the effect of epsilon values on learning performance

Prerequisites: Experiments 01-03 completed successfully
"""

# PyTorch 2.x Standard Practice Header
import os, random, numpy as np, torch

def setup_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# Proper device selection (CUDA > MPS > CPU)
device = torch.device(
    'cuda' if torch.cuda.is_available() 
    else 'mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    else 'cpu'
)
amp_enabled = torch.cuda.is_available()
setup_seed(42)

import gymnasium as gym
import matplotlib.pyplot as plt
from typing import List, Callable, Dict, Any, Tuple
import json
from dataclasses import dataclass

@dataclass
class PolicyResult:
    """Container for policy evaluation results"""
    name: str
    returns: List[float]
    mean_return: float
    std_return: float
    min_return: float
    max_return: float
    median_return: float

def make_env(env_id: str = "CartPole-v1", seed: int = 42) -> gym.Env:
    """Create and initialize environment with proper seeding"""
    env = gym.make(env_id)
    env.reset(seed=seed)
    env.action_space.seed(seed)
    env.observation_space.seed(seed)
    return env

def rollout_episode(env: gym.Env, policy_fn: Callable, max_steps: int = 500) -> Tuple[float, int]:
    """
    Run a single episode and return total reward and episode length
    
    Args:
        env: Gymnasium environment
        policy_fn: Policy function that maps observation to action
        max_steps: Maximum episode length
    
    Returns:
        Tuple of (total_reward, episode_length)
    """
    obs, _ = env.reset()
    total_reward = 0.0
    steps = 0
    
    for step in range(max_steps):
        action = policy_fn(obs)
        next_obs, reward, terminated, truncated, _ = env.step(action)
        total_reward += reward
        steps += 1
        obs = next_obs
        
        if terminated or truncated:
            break
    
    return total_reward, steps

def evaluate_policy(env_id: str, 
                   policy_fn: Callable, 
                   num_episodes: int = 100, 
                   seed: int = 42) -> List[float]:
    """
    Evaluate a policy over multiple episodes
    
    Args:
        env_id: Environment identifier
        policy_fn: Policy function
        num_episodes: Number of evaluation episodes
        seed: Random seed for reproducibility
    
    Returns:
        List of episode returns
    """
    env = make_env(env_id, seed)
    returns = []
    
    for ep in range(num_episodes):
        # Use different seed for each episode to get variety
        env.reset(seed=seed + ep)
        total_reward, _ = rollout_episode(env, policy_fn)
        returns.append(total_reward)
    
    env.close()
    return returns

# Policy Implementations

def random_policy(obs: np.ndarray) -> int:
    """Pure exploration: random action selection"""
    return np.random.randint(0, 2)

def heuristic_action(obs: np.ndarray) -> int:
    """
    Simple CartPole heuristic: push cart in direction to counter pole's fall
    This serves as our "greedy" baseline
    
    Args:
        obs: [position, velocity, angle, angular_velocity]
    
    Returns:
        Action (0: left, 1: right)
    """
    x, x_dot, theta, theta_dot = obs
    
    # Simple PD-like control signal
    # Push toward the side where the pole is falling
    control_signal = theta + 0.5 * theta_dot
    
    # If positive, push right; if negative, push left
    return 1 if control_signal > 0.0 else 0

def greedy_policy(obs: np.ndarray) -> int:
    """Pure exploitation: always take the heuristic action"""
    return heuristic_action(obs)

def epsilon_greedy_policy(epsilon: float):
    """
    Create an epsilon-greedy policy
    
    Args:
        epsilon: Exploration probability (0 <= epsilon <= 1)
    
    Returns:
        Policy function that does epsilon-greedy action selection
    """
    def policy(obs: np.ndarray) -> int:
        if np.random.random() < epsilon:
            # Explore: random action
            return np.random.randint(0, 2)
        else:
            # Exploit: use heuristic
            return heuristic_action(obs)
    
    return policy

def compare_policies(env_id: str = "CartPole-v1", 
                    num_episodes: int = 100, 
                    seed: int = 42) -> Dict[str, PolicyResult]:
    """
    Compare different exploration strategies
    
    Args:
        env_id: Environment to test on
        num_episodes: Number of episodes per policy
        seed: Random seed
    
    Returns:
        Dictionary mapping policy names to results
    """
    print(f"Evaluating policies on {env_id} with {num_episodes} episodes each...")
    
    # Define policies to test
    policies = {
        "Random (ε=1.0)": random_policy,
        "Greedy (ε=0.0)": greedy_policy,
        "ε-greedy (ε=0.1)": epsilon_greedy_policy(0.1),
        "ε-greedy (ε=0.2)": epsilon_greedy_policy(0.2),
        "ε-greedy (ε=0.5)": epsilon_greedy_policy(0.5),
    }
    
    results = {}
    
    for name, policy in policies.items():
        print(f"\nEvaluating {name}...")
        
        # Set seed for reproducible comparison
        setup_seed(seed)
        returns = evaluate_policy(env_id, policy, num_episodes, seed)
        
        # Calculate statistics
        result = PolicyResult(
            name=name,
            returns=returns,
            mean_return=float(np.mean(returns)),
            std_return=float(np.std(returns)),
            min_return=float(np.min(returns)),
            max_return=float(np.max(returns)),
            median_return=float(np.median(returns))
        )
        
        results[name] = result
        
        print(f"  Mean return: {result.mean_return:.2f} ± {result.std_return:.2f}")
        print(f"  Range: [{result.min_return:.0f}, {result.max_return:.0f}]")
        print(f"  Median: {result.median_return:.1f}")
    
    return results

def epsilon_sweep_analysis(env_id: str = "CartPole-v1", 
                          num_episodes: int = 50, 
                          seed: int = 42) -> Dict[float, PolicyResult]:
    """
    Analyze performance across different epsilon values
    
    Args:
        env_id: Environment to test on
        num_episodes: Number of episodes per epsilon value
        seed: Random seed
    
    Returns:
        Dictionary mapping epsilon values to results
    """
    print(f"\nRunning epsilon sweep analysis...")
    
    # Test different epsilon values
    epsilon_values = [0.0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0]
    results = {}
    
    for eps in epsilon_values:
        print(f"Testing ε = {eps:.2f}...")
        
        setup_seed(seed)
        policy = epsilon_greedy_policy(eps) if eps > 0 else greedy_policy
        returns = evaluate_policy(env_id, policy, num_episodes, seed)
        
        result = PolicyResult(
            name=f"ε={eps:.2f}",
            returns=returns,
            mean_return=float(np.mean(returns)),
            std_return=float(np.std(returns)),
            min_return=float(np.min(returns)),
            max_return=float(np.max(returns)),
            median_return=float(np.median(returns))
        )
        
        results[eps] = result
        print(f"  Mean return: {result.mean_return:.2f}")
    
    return results

def plot_policy_comparison(results: Dict[str, PolicyResult], save_path: str = None):
    """Create visualizations comparing different policies"""
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Exploration vs Exploitation Strategy Comparison', fontsize=16)
    
    policy_names = list(results.keys())
    means = [results[name].mean_return for name in policy_names]
    stds = [results[name].std_return for name in policy_names]
    
    # Plot 1: Mean returns with error bars
    x_pos = range(len(policy_names))
    bars = axes[0, 0].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue')
    axes[0, 0].set_title('Mean Return by Policy')
    axes[0, 0].set_xlabel('Policy')
    axes[0, 0].set_ylabel('Mean Return')
    axes[0, 0].set_xticks(x_pos)
    axes[0, 0].set_xticklabels(policy_names, rotation=45, ha='right')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Add value labels on bars
    for bar, mean in zip(bars, means):
        height = bar.get_height()
        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 1,
                       f'{mean:.1f}', ha='center', va='bottom')
    
    # Plot 2: Return distributions (box plot)
    return_data = [results[name].returns for name in policy_names]
    box_plot = axes[0, 1].boxplot(return_data, labels=policy_names, patch_artist=True)
    axes[0, 1].set_title('Return Distributions')
    axes[0, 1].set_xlabel('Policy')
    axes[0, 1].set_ylabel('Episode Return')
    axes[0, 1].tick_params(axis='x', rotation=45)
    axes[0, 1].grid(True, alpha=0.3)
    
    # Color the boxes
    colors = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow', 'lightpink']
    for patch, color in zip(box_plot['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    # Plot 3: Episode traces for first few policies
    for i, name in enumerate(list(policy_names)[:3]):
        returns = results[name].returns[:20]  # First 20 episodes
        axes[1, 0].plot(returns, 'o-', label=name, alpha=0.7, linewidth=1.5)
    
    axes[1, 0].set_title('Episode Return Traces (First 20 Episodes)')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Return')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Plot 4: Standard deviation comparison
    axes[1, 1].bar(x_pos, stds, alpha=0.7, color='orange')
    axes[1, 1].set_title('Return Variability (Standard Deviation)')
    axes[1, 1].set_xlabel('Policy')
    axes[1, 1].set_ylabel('Standard Deviation')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels(policy_names, rotation=45, ha='right')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Policy comparison plot saved to: {save_path}")

def plot_epsilon_sweep(results: Dict[float, PolicyResult], save_path: str = None):
    """Plot epsilon sweep results"""
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    epsilon_values = sorted(results.keys())
    means = [results[eps].mean_return for eps in epsilon_values]
    stds = [results[eps].std_return for eps in epsilon_values]
    
    # Plot 1: Mean return vs epsilon
    ax1.errorbar(epsilon_values, means, yerr=stds, marker='o', capsize=5, 
                linewidth=2, markersize=8, alpha=0.8)
    ax1.set_title('Mean Return vs Exploration Rate')
    ax1.set_xlabel('Epsilon (ε)')
    ax1.set_ylabel('Mean Return')
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(-0.05, 1.05)
    
    # Plot 2: Standard deviation vs epsilon
    ax2.plot(epsilon_values, stds, 'ro-', linewidth=2, markersize=8, alpha=0.8)
    ax2.set_title('Return Variability vs Exploration Rate')
    ax2.set_xlabel('Epsilon (ε)')
    ax2.set_ylabel('Standard Deviation')
    ax2.grid(True, alpha=0.3)
    ax2.set_xlim(-0.05, 1.05)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Epsilon sweep plot saved to: {save_path}")

def main():
    """Run exploration vs exploitation experiment"""
    print("="*60)
    print("Experiment 04: Exploration vs Exploitation")
    print("="*60)
    
    # Compare different policies
    policy_results = compare_policies(
        env_id="CartPole-v1",
        num_episodes=100,
        seed=42
    )
    
    # Epsilon sweep analysis
    epsilon_results = epsilon_sweep_analysis(
        env_id="CartPole-v1",
        num_episodes=50,
        seed=42
    )
    
    # Print detailed results
    print("\n" + "="*60)
    print("POLICY COMPARISON RESULTS")
    print("="*60)
    
    for name, result in policy_results.items():
        print(f"\n{name}:")
        print(f"  Mean return: {result.mean_return:.2f} ± {result.std_return:.2f}")
        print(f"  Median return: {result.median_return:.1f}")
        print(f"  Range: [{result.min_return:.0f}, {result.max_return:.0f}]")
    
    # Find best epsilon
    print("\n" + "="*60)
    print("EPSILON SWEEP RESULTS")
    print("="*60)
    
    best_eps = max(epsilon_results.keys(), key=lambda eps: epsilon_results[eps].mean_return)
    best_result = epsilon_results[best_eps]
    
    print(f"Best epsilon: {best_eps:.2f}")
    print(f"Best mean return: {best_result.mean_return:.2f} ± {best_result.std_return:.2f}")
    
    # Save results
    os.makedirs("results", exist_ok=True)
    
    # Convert results to JSON-serializable format
    policy_data = {}
    for name, result in policy_results.items():
        policy_data[name] = {
            'mean_return': result.mean_return,
            'std_return': result.std_return,
            'min_return': result.min_return,
            'max_return': result.max_return,
            'median_return': result.median_return,
            'returns': result.returns
        }
    
    epsilon_data = {}
    for eps, result in epsilon_results.items():
        epsilon_data[str(eps)] = {
            'epsilon': eps,
            'mean_return': result.mean_return,
            'std_return': result.std_return,
            'returns': result.returns
        }
    
    with open("results/policy_comparison.json", "w") as f:
        json.dump(policy_data, f, indent=2)
    
    with open("results/epsilon_sweep.json", "w") as f:
        json.dump(epsilon_data, f, indent=2)
    
    print("\nResults saved to:")
    print("  - results/policy_comparison.json")
    print("  - results/epsilon_sweep.json")
    
    # Create visualizations
    try:
        plot_policy_comparison(policy_results, "results/policy_comparison.png")
        plot_epsilon_sweep(epsilon_results, "results/epsilon_sweep.png")
    except ImportError:
        print("Matplotlib not available - skipping plots")
    except Exception as e:
        print(f"Plotting failed: {e}")
    
    # Key insights
    print("\n" + "="*60)
    print("KEY INSIGHTS:")
    print("="*60)
    print("1. Pure exploration (random) performs poorly")
    print("2. Pure exploitation (greedy) can get stuck in suboptimal behavior")
    print("3. Balanced exploration (ε-greedy) often performs best")
    print("4. Optimal ε depends on the problem and environment")
    print("5. Too much exploration hurts performance")
    print("6. Too little exploration prevents learning about better actions")
    
    print("\nExperiment 04 completed successfully!")
    return True

if __name__ == "__main__":
    main()